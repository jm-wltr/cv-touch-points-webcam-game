# Person Detection & Segmentation: Technical Analysis

## Overview
This document analyzes three different approaches to identifying people in video frames:
1. **Kristian's Approach**: Semantic segmentation with MediaPipe Holistic
2. **Jay's Approach**: Object detection with YOLOv8
3. **Alternative Methods**: Other computer vision techniques

---

## 1. KRISTIAN'S APPROACH: Semantic Segmentation

### What is Segmentation?

**Segmentation** = Classifying each pixel as "person" or "background"

```
Input Image (640x480):           Segmentation Mask (640x480):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ    â”‚             â”‚   0000  0000    â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ    â”‚             â”‚   0000  0000    â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚             â”‚    00000000     â”‚
â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â”‚  â”€â”€â”€â”€â”€â”€>    â”‚     111111      â”‚
â”‚    â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ     â”‚             â”‚    11111111     â”‚
â”‚   â–ˆâ–ˆ   â–ˆ   â–ˆâ–ˆ   â”‚             â”‚   11111111111   â”‚
â”‚  â–ˆâ–ˆ         â–ˆâ–ˆ  â”‚             â”‚  11111111111111 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 RGB image                       Binary mask
                                 0 = background
                                 1 = person
```

### How MediaPipe Holistic Segmentation Works

#### Architecture Overview

```
Input Frame (HÃ—WÃ—3 RGB)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MediaPipe Holistic Pipeline           â”‚
â”‚                                          â”‚
â”‚  1. Person Detection (BlazePose)        â”‚
â”‚     â””â”€> Find person bounding box        â”‚
â”‚                                          â”‚
â”‚  2. Landmark Estimation                 â”‚
â”‚     â””â”€> Extract 33 pose keypoints       â”‚
â”‚                                          â”‚
â”‚  3. Segmentation Network â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ (Kristian enables this)
â”‚     â””â”€> Pixel-wise person/background    â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Segmentation Mask (HÃ—WÃ—1 float32)
Values: 0.0 (background) to 1.0 (person)
```

#### Neural Network Architecture

MediaPipe uses a lightweight **U-Net-like** segmentation network:

```
Encoder (Downsampling):
Input (640Ã—480Ã—3)
    â†“ Conv + MaxPool
  320Ã—240Ã—64
    â†“ Conv + MaxPool
  160Ã—120Ã—128
    â†“ Conv + MaxPool
   80Ã—60Ã—256
    â†“ Conv + MaxPool
   40Ã—30Ã—512  â† Bottleneck

Decoder (Upsampling):
   40Ã—30Ã—512
    â†“ UpConv + Skip Connection
   80Ã—60Ã—256
    â†“ UpConv + Skip Connection
  160Ã—120Ã—128
    â†“ UpConv + Skip Connection
  320Ã—240Ã—64
    â†“ UpConv + Skip Connection
  640Ã—480Ã—1  â† Segmentation Mask
```

**Key Features:**
- **Skip connections**: Preserve fine details from encoder
- **Lightweight**: Optimized for mobile/real-time use
- **Single person**: Works best with one person in frame

#### Kristian's Post-Processing Pipeline

```python
def create_body_mask(segmentation_mask, frame_shape):
    # INPUT: Float mask [0.0, 1.0] from MediaPipe
    # OUTPUT: Clean binary mask [0, 255]

    # Step 1: Binarization
    binary_mask = (segmentation_mask > 0.5).astype(np.uint8) * 255
    # Threshold: 0.5 = 50% confidence

    # Step 2: Morphological Closing (fill small holes)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)

    # Step 3: Morphological Opening (remove small noise)
    binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)

    # Step 4: Gaussian Blur (smooth edges)
    binary_mask = cv2.GaussianBlur(binary_mask, (7, 7), 2)

    # Step 5: Re-threshold
    binary_mask = (binary_mask > 128).astype(np.uint8) * 255

    return binary_mask
```

#### Visual Pipeline Example

```
Original MediaPipe Mask       After Morphological Ops      After Gaussian Blur
(noisy, ragged edges)         (cleaner, filled holes)      (smooth silhouette)

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

#### Creating the Silhouette Effect

```python
def apply_body_overlay(frame, segmentation_mask, color=COLOR_BODY_FILL, opacity=0.7):
    # 1. Create clean mask
    mask = create_body_mask(segmentation_mask, frame.shape[:2])

    # 2. Create colored overlay
    overlay = frame.copy()
    colored_body = np.zeros_like(frame)
    colored_body[:] = (0, 255, 100)  # Green color

    # 3. Apply mask to colored body
    mask_3d = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR) / 255.0
    body_overlay = (colored_body * mask_3d).astype(np.uint8)

    # 4. Blend with original frame
    result = cv2.addWeighted(frame, 1 - opacity, body_overlay, opacity, 0)
    #              original   30%              overlay    70%

    # 5. Draw outline contours
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(result, contours, -1, (0, 200, 0), 2)

    return result
```

**Result:**
```
Original Frame        +    Green Overlay (70%)    =    Silhouette Effect
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Person in   â”‚            â”‚             â”‚            â”‚ Glowing     â”‚
â”‚ background  â”‚            â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚            â”‚ green       â”‚
â”‚             â”‚            â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚            â”‚ person      â”‚
â”‚             â”‚            â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚            â”‚ figure      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Inference time** | 25-40ms | CPU, includes pose + segmentation |
| **Accuracy** | ~95% IoU | Intersection over Union |
| **Edge quality** | High | After post-processing |
| **Multi-person** | âŒ No | Single person only |
| **Occlusion handling** | Moderate | Can lose person briefly |
| **Model size** | ~60 MB | Entire Holistic model |

### Strengths
âœ… **Pixel-perfect boundaries**: Exact person outline
âœ… **Real-time**: Fast enough for interactive applications
âœ… **Smooth edges**: Post-processing creates clean masks
âœ… **Integrated**: Comes with pose landmarks
âœ… **Temporal smoothing**: `smooth_segmentation=True` reduces flicker

### Weaknesses
âŒ **Single person only**: Can't distinguish multiple people
âŒ **No identity**: Can't tell which person is which
âŒ **Overhead**: Segmentation adds ~10-15ms
âŒ **Edge cases**: Struggles with extreme poses, tight clothing

---

## 2. JAY'S APPROACH: Object Detection

### What is Object Detection?

**Object Detection** = Finding bounding boxes around objects + classifying them

```
Input Image:                    Detection Output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     Person      â”‚            â”‚  â”‚ Person    â”‚  â”‚
â”‚    walking      â”‚  â”€â”€â”€â”€â”€â”€>   â”‚  â”‚ conf=0.89 â”‚  â”‚
â”‚                 â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚            â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               Box: (x1,y1,x2,y2)
                               Class: "person"
                               Confidence: 0.89
```

**No pixel-level mask** - just rectangular bounding boxes.

### How YOLO Object Detection Works

#### Architecture: YOLOv8n-pose

```
Input Frame (640Ã—640Ã—3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLOv8 Backbone (CSPDarknet)           â”‚
â”‚   â””â”€> Extract features at multiple scalesâ”‚
â”‚                                           â”‚
â”‚   Feature Pyramid Network (FPN)          â”‚
â”‚   â””â”€> Combine multi-scale features       â”‚
â”‚                                           â”‚
â”‚   Detection Head                          â”‚
â”‚   â””â”€> Predict boxes + classes            â”‚
â”‚                                           â”‚
â”‚   Pose Head (additional)                  â”‚
â”‚   â””â”€> Predict 17 keypoints per person    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Per-person outputs:
- Bounding box: [x1, y1, x2, y2]
- Confidence: 0.0-1.0
- Class: "person"
- Keypoints: 17Ã—3 (x, y, confidence)
```

#### YOLO Detection Process

```
1. Divide image into grid (e.g., 80Ã—80 cells)

â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤   Each cell predicts:
â”œâ”€â”¼â”€â”¼â–ˆâ”¼â–ˆâ”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤   - Is there a person here?
â”œâ”€â”¼â”€â”¼â–ˆâ”¼â–ˆâ”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤   - Where is the box?
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤   - How confident?
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤   - Where are keypoints?
â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜

2. Each cell outputs multiple predictions

3. Non-Maximum Suppression (NMS)
   - Remove duplicate/overlapping boxes
   - Keep highest confidence predictions
```

#### Jay's Detection Code

```python
# Run YOLO detection
results = model(frame, verbose=False)[0]

detections = []
if results.boxes is not None and results.keypoints is not None:
    for box, kps in zip(results.boxes.data, results.keypoints.data):
        x1, y1, x2, y2, conf, cls = box
        keypoints = kps.cpu().numpy()  # 17Ã—3 array

        detections.append({
            'bbox': [float(x1), float(y1), float(x2), float(y2)],
            'score': float(conf),
            'keypoints': keypoints  # COCO format: 17 points
        })
```

#### YOLO vs MediaPipe Output Comparison

**MediaPipe Segmentation:**
```
Output: 640Ã—480 float array
[
  [0.01, 0.02, 0.01, 0.89, 0.92, 0.88, ...],  â† Row 1
  [0.00, 0.01, 0.05, 0.91, 0.94, 0.90, ...],  â† Row 2
  ...
]
Every pixel has a value
```

**YOLO Detection:**
```
Output: List of detections
[
  {
    'bbox': [245, 120, 395, 460],      # Rectangle
    'score': 0.89,                      # Confidence
    'keypoints': [[x, y, conf], ...]    # 17 keypoints
  },
  {
    'bbox': [450, 100, 580, 440],      # Person 2
    'score': 0.92,
    'keypoints': [[x, y, conf], ...]
  }
]
Only bounding boxes, not pixels
```

### Multi-Person Tracking

Jay's tracker associates detections across frames:

```
Frame 1:                Frame 2:                Frame 3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”Œâ”€â”   â”Œâ”€â”  â”‚        â”‚ â”Œâ”€â”  â”Œâ”€â”   â”‚        â”‚â”Œâ”€â”    â”Œâ”€â”   â”‚
â”‚ â”‚Aâ”‚   â”‚Bâ”‚  â”‚  â”€â”€â”€>  â”‚ â”‚Aâ”‚  â”‚Bâ”‚   â”‚  â”€â”€â”€>  â”‚â”‚Aâ”‚    â”‚Bâ”‚   â”‚
â”‚ â””â”€â”˜   â””â”€â”˜  â”‚        â”‚ â””â”€â”˜  â””â”€â”˜   â”‚        â”‚â””â”€â”˜    â””â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tracking: A stays A, B stays B across frames
```

**Matching Algorithm:**
1. Compute IoU (box overlap) between old and new detections
2. Compute pose similarity (keypoint distances)
3. Combined score = 0.5 Ã— IoU + 0.5 Ã— pose_similarity
4. Hungarian algorithm finds optimal assignment
5. Unmatched detections â†’ try face recognition (InsightFace)
6. Still unmatched â†’ create new track

### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Inference time** | 30-60ms | CPU, per frame |
| **Accuracy** | ~85% mAP | Mean Average Precision |
| **Multi-person** | âœ… Yes | Unlimited people |
| **Identity tracking** | âœ… Yes | With face re-ID |
| **Occlusion handling** | Good | Track buffer keeps IDs |
| **Model size** | ~6 MB | YOLOv8n (nano) |

### Strengths
âœ… **Multi-person**: Handles any number of people
âœ… **Fast detection**: Optimized for real-time use
âœ… **Robust**: Works with occlusions, partial views
âœ… **Small model**: 6 MB vs 60 MB for Holistic
âœ… **Persistent IDs**: Re-identification with faces

### Weaknesses
âŒ **No pixel-level mask**: Only bounding boxes
âŒ **Can't create silhouettes**: No segmentation data
âŒ **False positives**: May detect non-people as people
âŒ **Bounding box limitations**: Includes background in box

---

## 3. COMPARISON: SEGMENTATION vs DETECTION

### Visual Comparison

```
Original Frame:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                â”‚
â”‚      â–ˆâ–ˆ                        â”‚
â”‚     â–ˆâ–ˆâ–ˆâ–ˆ        Person         â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       standing       â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     â”‚
â”‚    â–ˆâ–ˆ  â–ˆâ–ˆ                      â”‚
â”‚   â–ˆâ–ˆ    â–ˆâ–ˆ                     â”‚
â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MediaPipe Segmentation (Kristian):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 000000000000000000000000000000 â”‚ 0 = background
â”‚ 00000011100000000000000000000  â”‚ 1 = person
â”‚ 00000111110000000000000000000  â”‚
â”‚ 00001111111000000000000000000  â”‚ Perfect outline
â”‚ 000111111111000000000000000000 â”‚
â”‚ 00001110011100000000000000000  â”‚
â”‚ 000110000011000000000000000000 â”‚
â”‚ 000000000000000000000000000000 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YOLO Detection (Jay):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚ Bounding box
â”‚    â”‚      â–ˆâ–ˆ      â”‚            â”‚ includes
â”‚    â”‚     â–ˆâ–ˆâ–ˆâ–ˆ     â”‚            â”‚ background
â”‚    â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚            â”‚
â”‚    â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚            â”‚
â”‚    â”‚    â–ˆâ–ˆ  â–ˆâ–ˆ    â”‚            â”‚
â”‚    â”‚   â–ˆâ–ˆ    â–ˆâ–ˆ   â”‚            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     (x1,y1)    (x2,y2)
```

### Use Case Comparison

| Task | Kristian (Segmentation) | Jay (Detection) |
|------|------------------------|-----------------|
| **Visual effects** (silhouette) | âœ… Perfect | âŒ Can't do |
| **Background replacement** | âœ… Excellent | âŒ Can't do |
| **Body part tracking** | âœ… Good (1 person) | âœ… Excellent (multi-person) |
| **Counting people** | âŒ Limited (1 person) | âœ… Excellent |
| **Person re-ID** | âŒ No built-in | âœ… Yes (with face) |
| **Occlusion handling** | âš ï¸ Moderate | âœ… Good |
| **Multi-player game** | âŒ Can't distinguish | âœ… Perfect |

---

## 4. ALTERNATIVE METHODS

### Method 1: Traditional Background Subtraction

**Concept**: Subtract background to find foreground objects

```python
import cv2

# Create background subtractor
bg_subtractor = cv2.createBackgroundSubtractorMOG2(
    history=500,
    varThreshold=16,
    detectShadows=True
)

while True:
    ret, frame = cap.read()

    # Apply background subtraction
    fg_mask = bg_subtractor.apply(frame)

    # Clean up mask
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)

    # fg_mask is 0 (background) or 255 (foreground)
```

**How it works:**
```
Frame 1 (background):    Frame 2 (with person):    Difference:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚          â”‚             â”‚           â”‚             â”‚
â”‚  Furniture  â”‚    -     â”‚  Furniture  â”‚    =      â”‚             â”‚
â”‚             â”‚          â”‚   + Person  â”‚           â”‚   Person    â”‚
â”‚             â”‚          â”‚             â”‚           â”‚   mask      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Comparison:**

| Feature | MediaPipe | YOLO | Background Subtraction |
|---------|-----------|------|------------------------|
| Speed | ~30ms | ~50ms | **~5ms** âš¡ |
| Accuracy | High | High | **Low** âš ï¸ |
| Static camera | Optional | Optional | **Required** â— |
| Lighting changes | âœ… Robust | âœ… Robust | âŒ Sensitive |
| Cost | Free | Free | **Free** |

**Strengths:**
- Extremely fast (no neural network)
- Works for any moving object
- No model download needed

**Weaknesses:**
- Requires static camera (dealbreaker for handheld)
- Fails with lighting changes
- No person classification (detects any movement)
- No pose information

**Verdict:** âŒ **Not suitable for this game** (camera not static)

---

### Method 2: Mask R-CNN (Instance Segmentation)

**Concept**: Combines object detection + pixel-level segmentation

```python
import detectron2
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(
    "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(
    "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
)
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

predictor = DefaultPredictor(cfg)

# Inference
outputs = predictor(frame)
instances = outputs["instances"]

# Get masks for each person
masks = instances.pred_masks  # Boolean array per person
boxes = instances.pred_boxes
classes = instances.pred_classes
```

**Output:**
```
Person 1 Mask:          Person 2 Mask:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0001111000 â”‚        â”‚  0000000000 â”‚
â”‚  0011111100 â”‚        â”‚  0000111000 â”‚
â”‚  0011111100 â”‚        â”‚  0001111100 â”‚
â”‚  0001111000 â”‚        â”‚  0001111100 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Person 1                Person 2
```

**Comparison:**

| Feature | MediaPipe | YOLO | Mask R-CNN |
|---------|-----------|------|------------|
| Segmentation | âœ… Yes (1 person) | âŒ No | âœ… Yes (multi-person) |
| Inference time | 30ms | 50ms | **300-500ms** ðŸŒ |
| Multi-person | âŒ No | âœ… Yes | âœ… Yes |
| Model size | 60 MB | 6 MB | **~200 MB** |
| GPU required | âŒ No | âŒ No | âš ï¸ Highly recommended |

**Strengths:**
- **Best of both worlds**: Detection + segmentation
- Multi-person with individual masks
- High accuracy segmentation
- Can distinguish overlapping people

**Weaknesses:**
- **Very slow**: 300-500ms on CPU (10x slower than MediaPipe)
- Large model size
- No pose keypoints (would need separate model)
- Overkill for single-person game

**Verdict:** âš ï¸ **Good for multi-player with silhouettes**, but slow

---

### Method 3: DeepLab v3+ (Semantic Segmentation)

**Concept**: State-of-the-art semantic segmentation

```python
import tensorflow as tf

# Load pre-trained DeepLab model
model = tf.keras.models.load_model('deeplabv3_mobilenetv2.h5')

# Inference
input_tensor = preprocess(frame)
segmentation_mask = model.predict(input_tensor)

# segmentation_mask: (H, W) with class labels
# 0 = background, 15 = person (COCO classes)
person_mask = (segmentation_mask == 15)
```

**Architecture:**
```
Input â†’ MobileNetV2 Backbone
         â†“
      ASPP Module (Atrous Spatial Pyramid Pooling)
         â†“
      Decoder with Skip Connections
         â†“
    Segmentation Mask (21 classes)
```

**Comparison:**

| Feature | MediaPipe | DeepLab v3+ |
|---------|-----------|-------------|
| Accuracy | 95% IoU | **97% IoU** |
| Speed (CPU) | 30ms | **150-200ms** |
| Classes | Person only | **21 classes** |
| Edge quality | Good | **Excellent** |
| Multi-person | âŒ No | âš ï¸ All persons merged |

**Strengths:**
- Best segmentation quality
- Can segment background objects too (chairs, tables, etc.)
- Excellent edge detection

**Weaknesses:**
- Slower than MediaPipe (5-7x)
- Doesn't distinguish individual people (all "person" pixels are same class)
- No pose keypoints
- Larger model

**Verdict:** âš ï¸ **Better quality, but slower and no multi-person IDs**

---

### Method 4: Segment Anything Model (SAM)

**Concept**: Universal segmentation with prompts

```python
from segment_anything import sam_model_registry, SamPredictor

sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h.pth")
predictor = SamPredictor(sam)

predictor.set_image(frame)

# Prompt with a point (e.g., center of person)
input_point = np.array([[320, 240]])  # Center of frame
input_label = np.array([1])  # 1 = foreground

# Generate mask
masks, scores, logits = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True
)
```

**How it works:**
```
User clicks point     SAM segments         Result
on person          everything around
                     that point

    â†“                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ Person  â”‚
â”‚    â—    â”‚   â”€â”€â”€>   â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚   â”€â”€â”€>  â”‚ segment â”‚
â”‚  Person â”‚          â”‚â–‘Personâ–‘â–‘â”‚         â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Comparison:**

| Feature | MediaPipe | SAM |
|---------|-----------|-----|
| Automatic | âœ… Yes | âŒ Needs prompts |
| Accuracy | 95% | **98%+ IoU** |
| Speed | 30ms | **2-5 seconds** ðŸŒðŸŒ |
| Model size | 60 MB | **2.4 GB** ðŸ’¾ |
| Use case | Real-time | **Offline editing** |

**Strengths:**
- **Best segmentation quality** available
- Can segment anything (not just people)
- Handles complex scenes

**Weaknesses:**
- **Extremely slow**: 2-5 seconds per frame
- Requires prompts (clicks/boxes)
- Massive model (2.4 GB)
- Not suitable for real-time

**Verdict:** âŒ **Not suitable for real-time game**

---

### Method 5: GrabCut (Interactive Segmentation)

**Concept**: User-assisted foreground extraction

```python
import cv2
import numpy as np

# Initialize rectangle around person
rect = (50, 50, 450, 400)  # (x, y, width, height)

mask = np.zeros(frame.shape[:2], np.uint8)
bgd_model = np.zeros((1, 65), np.float64)
fgd_model = np.zeros((1, 65), np.float64)

# Run GrabCut
cv2.grabCut(
    frame,
    mask,
    rect,
    bgd_model,
    fgd_model,
    5,  # iterations
    cv2.GC_INIT_WITH_RECT
)

# Extract foreground
person_mask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')
```

**Comparison:**

| Feature | MediaPipe | GrabCut |
|---------|-----------|---------|
| Speed | 30ms | **500-1000ms** |
| Automatic | âœ… Yes | âŒ Needs rectangle |
| Accuracy | 95% | **90%** |
| Iterations | 1 (single pass) | **Multiple** |

**Strengths:**
- No deep learning model needed
- Good for photo editing
- Interactive refinement

**Weaknesses:**
- Very slow (graph-cut algorithm)
- Requires user input (bounding box)
- Lower accuracy than DL methods
- Not real-time

**Verdict:** âŒ **Not suitable for real-time**

---

### Method 6: Depth-Based Segmentation

**Concept**: Use depth camera to segment by distance

```python
import pyrealsense2 as rs
import numpy as np

# Configure RealSense pipeline
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)

pipeline.start(config)

while True:
    frames = pipeline.wait_for_frames()
    depth_frame = frames.get_depth_frame()
    color_frame = frames.get_color_frame()

    # Get depth data
    depth_image = np.asanyarray(depth_frame.get_data())

    # Segment by depth threshold
    person_mask = (depth_image > 500) & (depth_image < 2000)
    # Assumes person is 0.5-2m from camera
```

**Comparison:**

| Feature | MediaPipe | Depth Camera |
|---------|-----------|--------------|
| Speed | 30ms | **~16ms** âš¡ |
| Accuracy | 95% | **Varies** |
| Hardware | Webcam | **Depth camera** |
| Cost | Free | **$200-500** |
| Lighting | Important | **Irrelevant** |

**Strengths:**
- Very fast (no AI inference)
- Works in darkness
- Perfect for VR/AR applications
- No lighting dependency

**Weaknesses:**
- Requires specialized hardware (RealSense, Kinect, etc.)
- Limited range (0.3-10m typically)
- Depth noise in some conditions
- Overkill for 2D game

**Verdict:** âš ï¸ **Great if you have depth camera, overkill otherwise**

---

## 5. COMPREHENSIVE COMPARISON TABLE

### Performance

| Method | Speed (CPU) | Speed (GPU) | Accuracy | Model Size |
|--------|-------------|-------------|----------|------------|
| **MediaPipe Holistic** (Kristian) | 30-40ms | 15-20ms | 95% IoU | 60 MB |
| **YOLOv8n-pose** (Jay) | 30-60ms | 10-15ms | 85% mAP | 6 MB |
| Background Subtraction | 5ms | N/A | 60-70% | 0 MB |
| Mask R-CNN | 300-500ms | 50-80ms | 96% IoU | 200 MB |
| DeepLab v3+ | 150-200ms | 30-50ms | 97% IoU | 40 MB |
| SAM | 2-5s | 300-500ms | 98% IoU | 2.4 GB |
| GrabCut | 500-1000ms | N/A | 90% | 0 MB |
| Depth Camera | 16ms | N/A | 85-95% | 0 MB |

### Feature Matrix

| Method | Segmentation | Multi-Person | Pose | Real-Time | GPU Required |
|--------|--------------|--------------|------|-----------|--------------|
| **MediaPipe** | âœ… | âŒ | âœ… | âœ… | âŒ |
| **YOLO** | âŒ | âœ… | âœ… | âœ… | âŒ |
| BG Subtract | âœ… | âš ï¸ | âŒ | âœ… | âŒ |
| Mask R-CNN | âœ… | âœ… | âŒ | âŒ | âœ… |
| DeepLab | âœ… | âš ï¸ | âŒ | âš ï¸ | âš ï¸ |
| SAM | âœ… | âœ… | âŒ | âŒ | âœ… |
| GrabCut | âœ… | âŒ | âŒ | âŒ | âŒ |
| Depth | âœ… | âœ… | âŒ | âœ… | âŒ |

### Use Case Recommendations

| Your Goal | Best Method | Reason |
|-----------|-------------|--------|
| **Single player + silhouette** | MediaPipe Holistic | Perfect balance of speed and segmentation |
| **Multi-player game** | YOLO + Mask R-CNN | Detection + per-person masks (if GPU) |
| **Multi-player (no silhouette)** | YOLOv8-pose | Fast detection with pose tracking |
| **Best visual quality** | DeepLab v3+ or SAM | Highest segmentation accuracy |
| **Fastest possible** | Background Subtraction | Only if static camera |
| **VR/AR application** | Depth Camera | Best for 3D interactions |
| **Photo/video editing** | SAM or GrabCut | Not real-time, but interactive |

---

## 6. HYBRID APPROACHES

### Kristian's Game + Multi-Player

**Option A: YOLO + DeepLab**
```python
# Use YOLO for person detection
detections = yolo_model(frame)

# For each detected person:
for detection in detections:
    x1, y1, x2, y2 = detection['bbox']

    # Crop person region
    person_crop = frame[y1:y2, x1:x2]

    # Run DeepLab segmentation on crop
    mask = deeplab_model(person_crop)

    # Apply silhouette to this person only
    apply_overlay(frame, mask, color=person_color[detection.id])
```

**Trade-offs:**
- âœ… Multi-person silhouettes
- âœ… Individual colors per player
- âŒ Very slow (~200-300ms per frame)
- âŒ Complex pipeline

**Option B: MediaPipe + Person Re-ID**
```python
# Run MediaPipe on full frame (gets largest person)
results = holistic.process(frame)

# Use face recognition to identify which player
if results.face_landmarks:
    player_id = face_recognizer.identify(results.face_landmarks)

# Apply player-specific overlay
apply_overlay(frame, results.segmentation_mask, color=player_colors[player_id])
```

**Trade-offs:**
- âœ… Fast (still ~30-40ms)
- âœ… Beautiful silhouette quality
- âŒ Only one player at a time visible
- âŒ Must take turns or switch focus

### Jay's Game + Silhouettes

**Option: YOLO + Mask R-CNN**
```python
# YOLO for fast initial detection + tracking
yolo_detections = yolo_model(frame)
tracked_players = tracker.update(yolo_detections)

# Every N frames, run Mask R-CNN for segmentation
if frame_count % 5 == 0:  # Every 5 frames
    masks = maskrcnn_model(frame)
    # Cache masks for next 5 frames

# Use cached masks for rendering
for player in tracked_players:
    mask = masks[player.id]
    apply_overlay(frame, mask, player.color)
```

**Trade-offs:**
- âœ… Multi-person segmentation
- âœ… Good tracking from YOLO
- âš ï¸ Mask updates delayed (5 frames)
- âš ï¸ Requires GPU for reasonable speed

---

## 7. RECOMMENDATIONS

### For Current Kristian's Game
**Status Quo: Keep MediaPipe Holistic** âœ…

Reasons:
1. Perfect for single-player
2. Real-time segmentation
3. Smooth, beautiful silhouettes
4. CPU-friendly
5. Integrated with pose tracking

**No changes needed.**

### For Current Jay's Game
**Status Quo: Keep YOLO** âœ…

Reasons:
1. Multi-person is primary feature
2. Fast enough for tracking
3. Pose keypoints included
4. No need for segmentation (yet)

**If adding silhouettes:**
- Consider Mask R-CNN (requires GPU)
- Or use YOLO bounding boxes with alpha blending (fake silhouette)

### For Future Multi-Player + Silhouettes Game

**Recommended: YOLO + Mask R-CNN (Hybrid)**

```
Architecture:

  Frame Input
      â†“
  YOLOv8n-pose (30-60ms)
  â””â”€> Detect all people
  â””â”€> Get pose keypoints
      â†“
  Multi-Person Tracker
  â””â”€> Assign IDs
  â””â”€> Track across frames
      â†“
  Mask R-CNN (every 5 frames)
  â””â”€> Get segmentation masks
  â””â”€> Cache for interpolation
      â†“
  Render
  â””â”€> Overlay each person's mask
  â””â”€> Draw pose skeletons
  â””â”€> Game logic
```

**Requirements:**
- GPU (GTX 1060 or better)
- Python with PyTorch/Detectron2
- Total latency: ~100-150ms (still playable)

**Alternative (CPU-only):**
Use YOLO + simple bounding box overlays (no true segmentation):
```python
for player in players:
    x1, y1, x2, y2 = player.bbox
    # Create elliptical gradient overlay
    overlay = create_ellipse_gradient(x2-x1, y2-y1, player.color)
    blend(frame, overlay, (x1, y1))
```

Not as pretty as true segmentation, but real-time on CPU.

---

## 8. CONCLUSION

### Summary

**Kristian's silhouetting approach:**
- Uses MediaPipe Holistic's built-in segmentation
- Post-processes with morphological operations
- Creates smooth, beautiful silhouettes
- Perfect for single-player games

**Jay's detection approach:**
- Uses YOLO for bounding box detection
- Tracks multiple people with IDs
- No pixel-level masks, just rectangles
- Perfect for multi-player tracking

**Other methods:**
- **Mask R-CNN**: Best for multi-person segmentation (slow)
- **DeepLab**: Best segmentation quality (moderate speed)
- **SAM**: Best overall quality (offline only)
- **Depth cameras**: Best for 3D/VR (requires hardware)

### Final Recommendations

| Your Goal | Use This |
|-----------|----------|
| Kristian's game as-is | MediaPipe Holistic âœ… |
| Jay's game as-is | YOLOv8-pose âœ… |
| Multi-player + silhouettes (GPU) | YOLO + Mask R-CNN |
| Multi-player + silhouettes (CPU) | YOLO + fake gradients |
| Best possible quality | DeepLab v3+ or SAM |
| Fastest possible | Background subtraction (static camera) |

Both current implementations are optimal for their use cases! ðŸŽ¯
